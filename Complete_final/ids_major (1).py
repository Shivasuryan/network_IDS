# -*- coding: utf-8 -*-
"""ids_major.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11jPLuzr91eIXlMej5BLOaWXxAJjAU9L0
"""

import numpy as np
import pandas as pd
import seaborn as sns   #plot graphs
import matplotlib.pyplot as plt
from pandas.api.types import is_numeric_dtype #check whther is numeric datatype
from sklearn import tree
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder #scaling the values #strings to numbers
from sklearn.tree  import DecisionTreeClassifier

"""Load both train and test dataset"""

train_data = pd.read_csv('/content/sample_data/Train_data.csv')

test_data = pd.read_csv('/content/sample_data/Test_data.csv')

train_data.head()

train_data.info()

"""Should check with all those having missing values and lat"""

train_data.isnull().sum()

train_data.shape

train_data.describe(include='object')

print("Distribution in Training set: ")
print(train_data['class'].value_counts())

"""now have to convert coloumns having string to values using label encoder

"""

def enc(df):
    for col in df.columns:
        if df[col].dtype == 'object':
                label_encoder = LabelEncoder()
                df[col] = label_encoder.fit_transform(df[col])

enc(train_data)
enc(test_data)

train_data.drop(['num_outbound_cmds'], axis=1, inplace=True)
test_data.drop(['num_outbound_cmds'], axis=1, inplace=True)
train_data.head()

x_train = train_data.drop(['class'], axis=1)
y_train = train_data['class']

"""#feature scaling of datasets

"""

from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import RobustScaler
from sklearn.preprocessing import Normalizer
scale = MinMaxScaler()
x_train = scale.fit_transform(x_train)
test = scale.fit_transform(test_data)
x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, train_size=0.70, random_state=42)

x_train.shape

x_test.shape

y_train.shape

y_test.shape

import time

"""#Use DT classifier

entropy measures the impurity based on the distribution of classes in that node. In general, using 'entropy' can lead to more balanced splits in the tree
"""

clf = DecisionTreeClassifier(max_depth=5, criterion='entropy')

# Train the classifier on the training set
clf.fit(x_train, y_train)

# Predict the class labels for the training and testing sets
y_train_pred = clf.predict(x_train)
y_test_pred = clf.predict(x_test)

# Calculate the accuracy scores for the training and testing sets
dt_train = clf.score(x_train, y_train)
dt_test = clf.score(x_test, y_test)

print("Training accuracy score: {:.3f}".format(dt_train))
print("Testing accuracy score: {:.3f}".format(dt_test))

"""#Use KNN"""

from sklearn.neighbors import KNeighborsClassifier
clf = KNeighborsClassifier(n_neighbors=5)

# Train the classifier on the training set
clf.fit(x_train, y_train)

# Predict the class labels for the training and testing sets
y_train_pred = clf.predict(x_train)
y_test_pred = clf.predict(x_test)

# Calculate the accuracy scores for the training and testing sets
knn_train = clf.score(x_train, y_train)
knn_test = clf.score(x_test, y_test)

print("Training accuracy score: {:.3f}".format(knn_train))
print("Testing accuracy score: {:.3f}".format(knn_test))

from tabulate import tabulate
data = [["Decision Tree", dt_train, dt_test], ["KNN", knn_train, knn_test]]

col_names = ["Model", "Train Score", "Test Score"]
print(tabulate(data, headers=col_names, tablefmt="fancy_grid"))

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.model_selection import train_test_split

#rfc = RandomForestClassifier(n_estimators=100, max_depth=15, random_state=42)

# train the classifier
rfc.fit(x_train, y_train)

# make predictions on the test set
y_pred = rfc.predict(x_test)

# evaluate the performance of the classifier
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
print('Accuracy:', accuracy)
print('Confusion matrix:\n', conf_matrix)