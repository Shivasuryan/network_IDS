# -*- coding: utf-8 -*-
"""Final_1st part.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X2_eh89y9vPXIDnvre2icdK5wR2OmIPT

#classification
"""

from google.colab import drive
drive.mount("/content/drive")

import numpy as np
import pandas as pd
import seaborn as sns   #plot graphs
import matplotlib.pyplot as plt
from pandas.api.types import is_numeric_dtype #check whther is numeric datatype
from sklearn import tree
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder #scaling the values #strings to numbers
from sklearn.tree  import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix

import warnings
warnings.filterwarnings("ignore")

data = pd.read_csv('/content/drive/My Drive/cicids_dataset.csv')

data.head()

data.info()

data.describe()

data.shape

data.isnull().sum()

data.describe(include='object')

print("Distribution in Training Set:")
print(data['class'].value_counts())

def enc(df):
    for col in df.columns:
        if df[col].dtype == 'object':
                label_encoder = LabelEncoder()
                df[col] = label_encoder.fit_transform(df[col])

enc(data)

data.head()

data_new =data[np.isfinite(data).all(1)]

X = data_new.drop(['class'], axis=1)
y = data_new['class']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

std_X = StandardScaler()
X_train = std_X.fit_transform(X_train)  #be stored in form of array
X_test = std_X.fit_transform(X_test)

from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

nb_clf = GaussianNB(priors=None)
nb_clf.fit(X_train, y_train)

nb_pred = nb_clf.predict(X_test)
nb_acc = accuracy_score(y_test, nb_pred)

print("Naive Bayes accuracy: ", nb_acc)
sns.heatmap(confusion_matrix(y_test,nb_pred),annot=True,fmt="d")

from sklearn.metrics import classification_report, confusion_matrix
print(confusion_matrix(y_test, nb_pred))
print(classification_report(y_test, nb_pred))

from sklearn.tree import DecisionTreeClassifier
bft = DecisionTreeClassifier(criterion="entropy", splitter="best", max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0)

bft.fit(X_train, y_train)

bft_pred = bft.predict(X_test)

# Evaluate the performance of the classifier
accuracy = bft.score(X_test, y_test)
print("Accuracy:", accuracy)

print("Classification report of BFT:")
from sklearn.metrics import classification_report, confusion_matrix
print(confusion_matrix(y_test, bft_pred))
sns.heatmap(confusion_matrix(y_test,bft_pred),annot=True,fmt="d")
print(classification_report(y_test, bft_pred))

from sklearn.ensemble import RandomForestClassifier
# Create a Random Forest classifier with 100 trees
rfc = RandomForestClassifier(n_estimators=100, random_state=42)
rfc.fit(X_train, y_train)

rf_pred = rfc.predict(X_test)

accuracy = accuracy_score(y_test, rf_pred)

print("Accuracy:", accuracy)
print("Confusion matrix:\n", confusion_matrix(y_test, rf_pred))
sns.heatmap(confusion_matrix(y_test,rf_pred),annot=True,fmt="d")
print(classification_report(y_test, rf_pred))

from sklearn.neural_network import MLPClassifier
mlp = MLPClassifier(hidden_layer_sizes=(10,), max_iter=1000)

# Train the model
mlp.fit(X_train, y_train)

# Make predictions on the test set
y_pred = mlp.predict(X_test)

# Calculate the accuracy score and confusion matrix
accuracy = accuracy_score(y_test, y_pred)
confusion = confusion_matrix(y_test, y_pred)

print("Accuracy:", accuracy)
print("Confusion matrix:\n", confusion)
sns.heatmap(confusion_matrix(y_test, y_pred),annot=True,fmt="d")
print(classification_report(y_test, y_pred))

import os       #importing os to set environment variable
def install_java():
  !apt-get install -y openjdk-8-jdk-headless -qq > /dev/null      #install openjdk
  os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"     #set environment variable
  !java -version       #check java version
install_java()

!apt-get install libproj-dev proj-data proj-bin
!apt-get install libgeos-dev
!pip install cython
!pip install python-weka-wrapper3

import weka.core.jvm as jvm
from weka.core.converters import Loader
from weka.classifiers import Evaluation, Classifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report, confusion_matrix
# Start the JVM
jvm.start()


loader = Loader(classname="weka.core.converters.ArffLoader")
ids = loader.load_file("/content/drive/My Drive/test2.arff")

ids.class_is_last()

# Split the dataset into training and testing sets
train, test = ids.train_test_split(0.8)

# Create a J48 classifier
classifier = Classifier(classname="weka.classifiers.trees.J48")
#X_train = pd.DataFrame(X_train, columns = X.columns)

#X_test = pd.DataFrame(X_test, columns=X.columns)
# Train the classifier on the training set
classifier.build_classifier(train)

# Evaluate the classifier on the testing set


eval = Evaluation(train)
pred = eval.test_model(classifier, test)

accuracy = accuracy_score(y_test, y_pred)
confusion = confusion_matrix(y_test, y_pred)

# Print the accuracy of the model
print("Accuracy:", accuracy)
print("Confusion matrix:\n", confusion)
sns.heatmap(confusion_matrix(y_test, y_pred),annot=True,fmt="d")
print(classification_report(y_test, y_pred))